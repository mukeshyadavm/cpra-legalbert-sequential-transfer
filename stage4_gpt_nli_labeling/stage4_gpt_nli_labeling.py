# -*- coding: utf-8 -*-
"""stage4_gpt_nli_labeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/178GutgDaPttDtp4Hms0f1HAiygYHnvV0
"""

# 1. Install Dependencies

!pip install -q faiss-cpu sentence-transformers pandas

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import re


# 2. Premises (OPP-115)

premises_df = pd.read_csv("opp115_all_policies_combined.csv")
premises = premises_df["policy_text"].dropna().tolist()
print(f"Loaded Premises: {len(premises)}")


# 3. Tokenize Article 3

article3_path = "/content/article_3_full.txt"
with open(article3_path, "r") as f:
    article_text = f.read()

# Split into hypotheses (sentences > 20 chars)
hypotheses = [s.strip() for s in re.split(r'(?<=[.!?])\s+', article_text) if len(s.strip()) > 20]
print(f"Hypotheses: {len(hypotheses)}")

# 4. Load Model and Encode

model = SentenceTransformer('all-MiniLM-L6-v2')
BATCH_SIZE = 512

# Encode premises
premise_embeddings = model.encode(premises, batch_size=BATCH_SIZE, convert_to_numpy=True, normalize_embeddings=True)
# Encode hypotheses
hypothesis_embeddings = model.encode(hypotheses, batch_size=BATCH_SIZE, convert_to_numpy=True, normalize_embeddings=True)


#FAISS Index
d = premise_embeddings.shape[1]
index = faiss.IndexFlatIP(d)
index.add(premise_embeddings)

# 6. Search and Threshold Filtering

SIM_THRESHOLD = 0.6
results = []

# Query all hypotheses
scores, indices = index.search(hypothesis_embeddings, k=len(premises))  # Search all premises

for hypo_idx, (score_list, idx_list) in enumerate(zip(scores, indices)):
    for score, idx in zip(score_list, idx_list):
        if score >= SIM_THRESHOLD:
            results.append((premises[idx], hypotheses[hypo_idx], float(score)))

#  DataFrame
df_results = pd.DataFrame(results, columns=["premise", "hypothesis", "similarity_score"])

output_path = "/content/drive/MyDrive/NLI_Results/article3_nli_semantic_pairs_faiss.csv"
df_results.to_csv(output_path, index=False)
print(f" Saved {len(df_results)} pairs to: {output_path}")



# 1) Install & Imports

!pip install -q openai pandas

import os, json, time
import pandas as pd
from openai import OpenAI

# 2) Config

client = OpenAI(api_key=os.getenv(""))

INPUT_PATH  = "/content/drive/MyDrive/NLI_Results/article3_nli_semantic_pairs_faiss.csv"
OUTPUT_PATH = "/content/drive/MyDrive/NLI_Results/article3_nli_semantic_pairs_labeled.csv"

MODEL       = "gpt-4-turbo"
BATCH_SIZE  = 40
MAX_LEN     = 1200
SLEEP_BASE  = 2.0

# 3) Load, De-dupe, Resume

raw = pd.read_csv(INPUT_PATH)[["premise", "hypothesis"]].dropna()
raw = raw.drop_duplicates().reset_index(drop=True)

if os.path.exists(OUTPUT_PATH):
    prev = pd.read_csv(OUTPUT_PATH)
    if "label" not in prev.columns:
        prev["label"] = None
    merged = raw.merge(prev, on=["premise", "hypothesis"], how="left")
    df = merged[["premise", "hypothesis", "label"]].copy()
    print(f"Resuming: {df['label'].notna().sum()} labeled / {len(df)} total")
else:
    df = raw.copy()
    df["label"] = None
    print(f"Starting fresh: {len(df)} pairs")


# 4) Helpers

def trunc(s, n=MAX_LEN):
    s = str(s)
    return s if len(s) <= n else s[:n] + "â€¦"

def make_batches(indices, size=BATCH_SIZE):
    for i in range(0, len(indices), size):
        yield indices[i:i+size]

def build_items(subdf, idxs):
    return [
        {"id": int(i),
         "premise": trunc(subdf.at[i, "premise"]),
         "hypothesis": trunc(subdf.at[i, "hypothesis"])}
        for i in idxs
    ]

def build_prompt(items):
    sys_msg = (
        "You are an expert legal Natural Language Inference (NLI) classifier. "
        "Each item has a legal 'premise' and 'hypothesis'. "
        "Classify their relationship as one of: Entailment, Contradiction, Neutral. "
        "Definitions:\n"
        "- Entailment: Premise guarantees the hypothesis is true under law.\n"
        "- Contradiction: Premise conflicts with or negates the hypothesis.\n"
        "- Neutral: Neither entails nor contradicts; unrelated or insufficient info.\n\n"
        "Respond ONLY in JSON Lines format, one object per line:\n"
        '{"id": <int>, "label": "<Entailment|Contradiction|Neutral>"}\n'
        "No prose, no explanations, no extra keys."
    )
    user_msg = {
        "role": "user",
        "content": json.dumps({"items": items}, ensure_ascii=False)
    }
    return sys_msg, user_msg

def parse_jsonl(text):
    results = {}
    for line in text.splitlines():
        if not line.strip():
            continue
        try:
            obj = json.loads(line)
            _id = int(obj["id"])
            lab = str(obj["label"]).strip().lower()
            if lab.startswith("entail"): lab = "Entailment"
            elif lab.startswith("contra"): lab = "Contradiction"
            else: lab = "Neutral"
            results[_id] = lab
        except Exception as e:
            print(f"[Parse Error] {e} | Line: {line}")
    return results

def label_batch(items, attempt=1):
    sys_msg, user_msg = build_prompt(items)
    try:
        resp = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": sys_msg},
                user_msg
            ],
            temperature=0,
        )
        return parse_jsonl(resp.choices[0].message.content)
    except Exception as e:
        wait = SLEEP_BASE * attempt
        print(f"[Warn] Batch error: {e} | Retrying in {wait:.1f}s (attempt {attempt})")
        time.sleep(wait)
        if attempt < 5:
            return label_batch(items, attempt+1)
        else:
            raise

def save_progress(frame):
    frame.to_csv(OUTPUT_PATH, index=False)


# 5) Batched Labeling

unlabeled_idxs = [i for i, v in enumerate(df["label"]) if pd.isna(v)]
print(f"To label now: {len(unlabeled_idxs)}")

start = time.time()
done = 0

for chunk in make_batches(unlabeled_idxs, BATCH_SIZE):
    items = build_items(df, chunk)
    results = label_batch(items)

    for i in chunk:
        df.at[i, "label"] = results.get(i, "Neutral")  # fallback

    done += len(chunk)
    if done % (BATCH_SIZE * 2) == 0 or done == len(unlabeled_idxs):
        save_progress(df)
        elapsed = time.time() - start
        rate = done / max(elapsed, 1)
        eta = (len(unlabeled_idxs) - done) / max(rate, 1e-6)
        print(f"Progress: {done}/{len(unlabeled_idxs)} | ~{rate:.2f} rows/s | ETA ~{eta/60:.1f} min")

# Final save
save_progress(df)
print(f"\nSaved labeled file at: {OUTPUT_PATH}")
print(f"Total labeled rows: {df['label'].notna().sum()} / {len(df)}")