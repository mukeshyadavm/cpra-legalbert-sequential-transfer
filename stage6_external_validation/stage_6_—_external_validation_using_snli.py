# -*- coding: utf-8 -*-
"""Stage 6 — External Validation using SNLI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_MRMoxwG0DiRZyEZ-Dd7ph8Ld_bFGStW
"""

import os
print(os.getcwd())

from google.colab import drive
drive.mount('/mnt/gdrive')

from datasets import load_dataset
import pandas as pd

# Load the dataset
dataset = load_dataset("snli")

from datasets import load_dataset
import pandas as pd
import matplotlib.pyplot as plt

# Load SNLI
dataset = load_dataset("snli")

# Combine splits into one DataFrame
df_all = pd.concat([
    dataset["train"].to_pandas(),
    dataset["validation"].to_pandas(),
    dataset["test"].to_pandas()
])

# Label mapping
label_map = {0: "entailment", 1: "neutral", 2: "contradiction", -1: "hidden/missing"}

# Count labels
label_counts = df_all["label"].map(label_map).value_counts()

# Print distribution
print("Total rows:", len(df_all))
print("\nLabel distribution:")
print(label_counts)

# Plot distribution
plt.figure(figsize=(6,4))
label_counts.plot(kind="bar")
plt.title("SNLI Overall Label Distribution")
plt.xlabel("Label")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.show()

from datasets import load_dataset
import pandas as pd

# Load dataset
dataset = load_dataset("snli")

# Convert train split to pandas
df = dataset["train"].to_pandas()

# Label map
label_map = {0: "entailment", 1: "neutral", 2: "contradiction", -1: "hidden/missing"}

# Select only premise, hypothesis, and label
sample_df = df[["premise", "hypothesis", "label"]].head(10)
sample_df["label"] = sample_df["label"].map(label_map)

print(sample_df)

from datasets import load_dataset
import pandas as pd

# Load SNLI
dataset = load_dataset("snli")
df_test = dataset["test"].to_pandas()

# Map labels
label_map = {0: "entailment", 1: "neutral", 2: "contradiction", -1: "hidden/missing"}
df_test["label"] = df_test["label"].map(label_map)

# Select only the needed columns
df_test_simple = df_test[["premise", "hypothesis", "label"]]

# Display first 10 rows in clean 3-column format
print(df_test_simple.head(10).to_string(index=False))

import os
os.environ["OPENAI_API_KEY"] = "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"

from openai import OpenAI
client = OpenAI(api_key=os.getenv("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"))

!pip install -q openai datasets pandas scikit-learn

import os, json, time
import pandas as pd
from openai import OpenAI
from datasets import load_dataset
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 2) Config
client = OpenAI(api_key=os.getenv("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"))
OUTPUT_PATH = "/mnt/gdrive/MyDrive/NLI_Results/snli_test_labeled.csv"

MODEL       = "gpt-4-turbo"
BATCH_SIZE  = 40
MAX_LEN     = 1200
SLEEP_BASE  = 2.0

# 3) Load SNLI test split
dataset = load_dataset("snli")
df = dataset["test"].to_pandas()[["premise", "hypothesis", "label"]]

# Map gold labels
label_map = {0: "Entailment", 1: "Neutral", 2: "Contradiction", -1: "Neutral"}
df["label_gold"] = df["label"].map(label_map)
df = df.drop(columns=["label"])
df["label_gpt"] = None

print(f"Loaded SNLI test set: {len(df)} rows")

# 4) Helpers
def trunc(s, n=MAX_LEN):
    s = str(s)
    return s if len(s) <= n else s[:n] + "…"

def make_batches(indices, size=BATCH_SIZE):
    for i in range(0, len(indices), size):
        yield indices[i:i+size]

def build_items(subdf, idxs):
    return [
        {"id": int(i),
         "premise": trunc(subdf.at[i, "premise"]),
         "hypothesis": trunc(subdf.at[i, "hypothesis"])}
        for i in idxs
    ]

def build_prompt(items):
    sys_msg = (
        "You are an expert Natural Language Inference (NLI) classifier. "
        "Each item has a 'premise' and a 'hypothesis'. "
        "Classify their relationship as one of: Entailment, Contradiction, Neutral.\n"
        "- Entailment: Premise guarantees the hypothesis.\n"
        "- Contradiction: Premise conflicts with the hypothesis.\n"
        "- Neutral: Not enough info / unrelated.\n\n"
        "Respond ONLY in JSON Lines format, one object per line:\n"
        '{"id": <int>, "label": "<Entailment|Contradiction|Neutral>"}'
    )
    user_msg = {
        "role": "user",
        "content": json.dumps({"items": items}, ensure_ascii=False)
    }
    return sys_msg, user_msg

def parse_jsonl(text):
    results = {}
    for line in text.splitlines():
        if not line.strip():
            continue
        try:
            obj = json.loads(line)
            _id = int(obj["id"])
            lab = str(obj["label"]).strip().lower()
            if lab.startswith("entail"): lab = "Entailment"
            elif lab.startswith("contra"): lab = "Contradiction"
            else: lab = "Neutral"
            results[_id] = lab
        except Exception as e:
            print(f"[Parse Error] {e} | Line: {line}")
    return results

def label_batch(items, attempt=1):
    sys_msg, user_msg = build_prompt(items)
    try:
        resp = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": sys_msg},
                user_msg
            ],
            temperature=0,
        )
        return parse_jsonl(resp.choices[0].message.content)
    except Exception as e:
        wait = SLEEP_BASE * attempt
        print(f"[Warn] Batch error: {e} | Retrying in {wait:.1f}s (attempt {attempt})")
        time.sleep(wait)
        if attempt < 5:
            return label_batch(items, attempt+1)
        else:
            raise

def save_progress(frame):
    frame.to_csv(OUTPUT_PATH, index=False)

# 5) Batched Labeling
unlabeled_idxs = [i for i, v in enumerate(df["label_gpt"]) if pd.isna(v)]
print(f"To label now: {len(unlabeled_idxs)}")

start = time.time()
done = 0

for chunk in make_batches(unlabeled_idxs, BATCH_SIZE):
    items = build_items(df, chunk)
    results = label_batch(items)

    for i in chunk:
        df.at[i, "label_gpt"] = results.get(i, "Neutral")  # fallback if missing

    done += len(chunk)
    if done % (BATCH_SIZE * 2) == 0 or done == len(unlabeled_idxs):
        save_progress(df)
        elapsed = time.time() - start
        rate = done / max(elapsed, 1)
        eta = (len(unlabeled_idxs) - done) / max(rate, 1e-6)
        print(f"Progress: {done}/{len(unlabeled_idxs)} | ~{rate:.2f} rows/s | ETA ~{eta/60:.1f} min")

# Final save
save_progress(df)
print(f"\nSaved labeled file at: {OUTPUT_PATH}")
print(f"Total labeled rows: {df['label_gpt'].notna().sum()} / {len(df)}")

# 6) Evaluation
print("\n=== GPT vs Gold Evaluation ===")
print("Accuracy:", accuracy_score(df["label_gold"], df["label_gpt"]))
print("\nClassification Report:\n", classification_report(df["label_gold"], df["label_gpt"]))
print("\nConfusion Matrix:\n", confusion_matrix(df["label_gold"], df["label_gpt"]))

import os, json, time
import pandas as pd
from openai import OpenAI
from datasets import load_dataset
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import pandas as pd
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load saved labeled file
df = pd.read_csv("/mnt/gdrive/MyDrive/NLI_Results/snli_test_labeled.csv")

# Ensure correct column names exist
y_true = df["label_gold"]
y_pred = df["label_gpt"]

# Final evaluation
print("\n=== Final GPT vs Gold Evaluation ===")
print("Accuracy:", accuracy_score(y_true, y_pred))
print("\nClassification Report:\n", classification_report(y_true, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_true, y_pred))

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# ------------------------------------------------------------------
# Confusion matrix in (Contradiction, Entailment, Neutral) order
# rows = true labels, cols = predicted labels
# ------------------------------------------------------------------
conf_mat = np.array([
    [3122,    5,  110],   # True Contradiction
    [  30, 3107,  231],   # True Entailment
    [ 318,  362, 2715]    # True Neutral
])

labels = ["Contradiction", "Entailment", "Neutral"]

# ------------------------------------------------------------------
# Plot
# ------------------------------------------------------------------
plt.figure(figsize=(3.4, 2.8), dpi=300)  # slightly smaller to avoid text clipping

ax = sns.heatmap(
    conf_mat,
    annot=True,
    fmt="d",
    cmap="Greys",
    xticklabels=labels,
    yticklabels=labels,
    cbar_kws={"shrink": 0.75}
)

# Axis labels
ax.set_xlabel("Predicted label", fontsize=9)
ax.set_ylabel("True label", fontsize=9)

# Title kept short so it doesn’t dominate the figure
ax.set_title("Confusion Matrix – GPT vs Gold (SNLI External Validation)", fontsize=9, pad=6)

# Make tick labels a bit smaller
ax.set_xticklabels(labels, fontsize=8)
ax.set_yticklabels(labels, fontsize=8, rotation=0)

plt.tight_layout()
plt.savefig("confusion_matrix_snli_ieee.png", dpi=600, bbox_inches="tight")
plt.show()